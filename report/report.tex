\documentclass[12pt, a4paper]{article}

\nonstopmode
\usepackage{algorithm}
% \usepackage{algcompatible}
\usepackage{bm}
\usepackage{bbm}
\usepackage{algpseudocode}
\usepackage{amsmath} % flere matematikkommandoer
\usepackage{amssymb} % flere matematikkommandoer
\usepackage{arydshln}
\usepackage[utf8]{inputenc} % æøå
\usepackage[T1]{fontenc} % mere æøå
\usepackage{verbatim} % så man kan skrive ren tekst
\usepackage[all]{xy} % den sidste (avancerede) formel i dokumentet
\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides
\usepackage{graphicx}              % to include figures
\usepackage{amsfonts}              % for blackboard bold, etc
\usepackage{amsthm}                % better theorem environments
\usepackage{graphicx}
\usepackage{caption}
\usepackage{bm}
\usepackage{pgf}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{listings}
\usepackage{subcaption}
\usepackage[inline]{enumitem}
\usepackage[round]{natbib}
\usepackage{breqn}
\usepackage{tikz}
\usetikzlibrary{arrows,automata,fit,positioning, shapes, calc, fadings}

% various theorems, numbered by section

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}

\DeclareMathOperator{\id}{id}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\nn}{\mathcal{N}}
\newcommand{\uu}{\mathcal{U}}
\newcommand{\bb}{\mathcal{B}}
\newcommand{\ee}{\mathrm{e}}
\newcommand{\rd}[1]{\mathrm{#1}}
\newcommand{\bd}[1]{\mathbf{#1}}  % for bolding symbols
\newcommand{\RR}{\mathbb{R}}      % for Real numbers
\newcommand{\ZZ}{\mathbb{Z}}      % for Integers
\newcommand{\PP}{\mathbb{P}}      % for Prob
\newcommand{\EE}{\mathbb{E}}      % for Expectation
\newcommand{\II}{\mathbbm{1}}      % for Indicator fun
\newcommand{\NN}{\mathbb{N}}      % for Prob
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\eqsys}[2]{ \left[\!\!
    \begin{array}{#1} #2
    \end{array}
    \!\!\right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}
\newcommand{\sint}[1]{\shortintertext{#1}}
\newcommand{\CTL}[1]{\:\textrm{#1}\:}
\newcommand{\sat}{\:\:\textrm{\textbf{sat}}\:\:}
\newcommand{\pr}{\textrm{Pr}}
\newcommand{\grw}{m_{\mathcal{H}}}
\newcommand{\dvc}{d_{VC}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{Lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newcommand{\lstbg}[3][0pt]{{\fboxsep#1\colorbox{#2}{\strut #3}}}
\newcommand{\kl}{\textrm{kl}}
\newcommand{\KL}{\textrm{KL}}
\newcommand{\hyp}{\mathcal{H}}
\lstdefinelanguage{diff}{
  basicstyle=\ttfamily\scriptsize,
  morecomment=[f][\lstbg{red!20}]-,
  morecomment=[f][\lstbg{green!20}]+,
  morecomment=[f][\textit]{@@},
  %morecomment=[f][\textit]{---},
  %morecomment=[f][\textit]{+++},
}

\newenvironment{gamedef}
{%
 \par\medskip\noindent    
 \textbf{Game Defintion:} \\ % HEADLINE
 \noindent For $t = 1,2,...$:
    \begin{enumerate}[leftmargin=3em,labelsep=1em,beginpenalty=10000]
    \setlength{\itemsep}{0.2em}
    \setlength{\parskip}{0.5em}
    \setlength{\parsep}{0pt}
}
{%
    \end{enumerate}
}
\definecolor{eclipseBlue}{RGB}{42,0.0,255}
\definecolor{eclipseGreen}{RGB}{63,127,95}
\definecolor{eclipsePurple}{RGB}{127,0,85}

\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}

\lstset{
  language={haskell},
  basicstyle=\ttfamily, % Global Code Style
  captionpos=b, % Position of the Caption (t for top, b for bottom)
  extendedchars=true, % Allows 256 instead of 128 ASCII characters
  tabsize=2, % number of spaces indented when discovering a tab
  columns=fixed, % make all characters equal width
  keepspaces=true, % does not ignore spaces to fit width, convert tabs to spaces
  showstringspaces=false, % lets spaces in strings appear as real spaces
  breaklines=true, % wrap lines if they don't fit
  frame=trbl, % draw a frame at the top, right, left and bottom of the listing
  frameround=tttt, % make the frame round at all four corners
  framesep=4pt, % quarter circle size of the round corners
  numbers=left, % show line numbers at the left
  numberstyle=\small\ttfamily, % style of the line numbers
  commentstyle=\slshape\bfseries\color{eclipseGreen}, % style of comments
  keywordstyle=\bfseries\color{eclipsePurple}, % style of keywords
  stringstyle=\color{eclipseBlue}, % style of strings
  emphstyle=[1]{\color{eclipseBlue}},
  moredelim=**[is][\color{red}]{@@}{@@}
}
\renewcommand{\thesubsubsection}{\thesection.\alph{subsubsection}}
\begin{document}
\author{Chi Pham\\William Sprent}
\title{PFP Exam Project\\Improving Rasterific}
\maketitle

\tableofcontents
\clearpage

\section{Introduction}
% Noget om hvad projektet går ud på, måske hovedresultater
If a sequential program exhibits certain characteristics, it may be viable to apply parallelisation strategies to it in hopes of
 obtaining improved execution times.
In this report, we present a handful of parallelisation experiments which we have 
on the Rasterific\footnote{\url{https://github.com/Twinside/Rasterific}} rasterisation library.

 We did not manage to achieve any valuable speedup. However, we have made some observations which may guide
 further work in parallelising the library.
\section{Background}\label{background}

This section describes the structure of the Rasterific library and the tools we will use for parallelisation.

\subsection{Rasterific}
\begin{figure}[h!]
  \centering
  \includegraphics[width=.4\linewidth]{../flakes}
  \caption{The result of the ``snowflake'' benchmark.}
  \label{fig:snowflakes}
\end{figure}

Rasterific is a vector graphics rasterization engine written in Haskell. In short, this means it takes a series
of instructions in form of primitive geometry (circles, rectangles etc.) and/or line strokes along with some texturing options and produces a raster image like the
 one in Figure \ref{fig:snowflakes}. It is based
 on a rasterizer called Gezira\footnote{\url{https://github.com/damelang/gezira}} written in the Nile programming language\footnote{\url{https://github.com/damelang/nile}}.
%What is rasterific, overall pipeline, levels of granularity, main data types. Needs some kind of drawing or graph, to be referenced later in Section \ref{experiments}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.6\linewidth]{../rasterific-pipeline}
  \caption{Approximation of the Rasterific pipeline.}
  \label{fig:rasterific-pipeline}
\end{figure}

Figure \ref{fig:rasterific-pipeline} shows our basic understanding of the general pipeline in Rasterific. The user of the library defines a bunch of vector shapes by calling the
API which are represented internally as a series of commands in a free monad. When this is interpreted, a list of \texttt{DrawOrder} objects are generated which contain lists
of primitives (Bezier curves - linear, quadratic, and cubic) which are the pumped through a rasterization pipeline (the left wing in Figure \ref{fig:rasterific-pipeline}). The rasterization
pipeline decomposes the primitives into raster lines, sorts them, and finally combines them. Finally, a filling strategy and some texture is applied and we have an image.

Based on a discussion in Rasterifics Issue Tracker\footnote{\url{https://github.com/Twinside/Rasterific/issues/32}}, we have focused mainly on the full rasterization pipeline of
primitives, this includes the following:
\begin{enumerate}
\item the decomposition of primitives into \texttt{EdgeSample}s, a very fine-grained task with potential for constant span;
\item the sorting of \texttt{EdgeSample}s, a known problem with somewhat limited potential for parallism; and,
\item the combination of \texttt{EdgeSample}s into \texttt{CoverageSpan}s, a somewhat more complicated and course grained task with the highest potential for speedup at face value.
\end{enumerate}

It's worth noting that the Gezira library is parallel to some degree. It may be fruitful to base any parallelization efforts in Rasterific on how it is achieved in Gezira. However,
since Gezira is written in Nile and compiled into C with a heavy runtime which we would need to understand to understand when the rasterizer is multithreaded, we chosen to instead
 base our work on the comments of Rasterific's author.
%Måske noget om benchmarks?
\subsection{Parallel Haskell}
For this project, we have spent most of our time working with the Par Monad -- besides with sorting where we end up making use of the primitives in \texttt{Control.Parallel}.
 This is mainly based on ease of use. We could as well have chosen to work with Strategies, and it may
have been preferable considering the fine granularity of some of our tasks, but we feel more comfortable working with the more simple API from \texttt{Monad.Par}.
However, to make use of some of the more powerful analysis tools in ThreadScope, we make use of the spark scheduler for the Par Monad
\footnote{\url{https://hackage.haskell.org/package/monad-par-0.3.4.8/docs/Control-Monad-Par-Scheds-Sparks.html}}.

The main consequence of this choice is that the tools from the Par Monad package are hyperstrict, so we are forced to force a lot of lazy computations to enable parallelism -- pun not intended.


%Focus on Par monad (forced evaluation as opposed to lazy). Haven't needed concurrent primitives because most of the code lends itself well to parallel primitives (with the possible exception of sorting). Also it was easiest to use ...


\section{Experiments}\label{experiments}

This section describes our experiments in parallelising Rasterific, as well as our results. Our approach is fairly modular -- we have identified a number of places that could be parallelised, implemented our changes, and evaluated the performance isolated from the other instances.

We have made parallelisation attempts on a number of different granularities, parallelising across \texttt{EdgeSamples} (fine) to \texttt{DrawOrders} (coarse). See Figure \ref{fig:rasterific-pipeline-flot} for an approximation of the granularity, the numbers denoting the section in which the experiment is described.

\begin{figure}[H]
  \centering
  \includegraphics[width=.6\linewidth]{../rasterific-pipeline-flot}
  \caption{Parallelisation granularity in the Rasterific pipeline.}
  \label{fig:rasterific-pipeline-flot}
\end{figure}


\subsection{Benchmarks}
We will make references to some benchmarks further down. This is a short recap of which benchmarks exist in Rasterific and which ones we have added.
\begin{itemize}
\item \textbf{Snowflake:} Draws a series of transparent snowflakes on a background. See Figure \ref{fig:snowflakes}. Was added to the Rasterific project
   shortly before we started our project.
\item \textbf{BigSquare:} Draws a big pink snowflake (named for legacy reasons). We have added this to benchmark a single call to the rasteriser containing many \texttt{Primitives}. See Figure \ref{fig:bigsquare-benchmark}.
\item \textbf{Lines:} Draws a large number of randomly colored lines on a background. See Figure \ref{fig:lines-benchmark}.
  We have added this to benchmark line primitive decomposition.
\end{itemize}
\begin{figure}[h!]
  \centering
  \includegraphics[width=.2\linewidth]{../bigsquare}
  \caption{The output of the \texttt{bigsquare} benchmark.}
  \label{fig:bigsquare-benchmark}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=.4\linewidth]{../lines}
  \caption{The output of the \texttt{lines} benchmark.}
  \label{fig:lines-benchmark}
\end{figure}

\subsection{Primitives}
Before rasterization, Rasterific works with three different primitive shapes:
lines, quadratic bezier curves, and cubic bezier curves. Each of these are decomposed
to an internal type representing raster lines:
\begin{lstlisting}[caption={\texttt{EdgeSample} type -- represents a raster line.}]
data EdgeSample = EdgeSample
  { _sampleX     :: {-# UNPACK #-} !Float -- ^ Horizontal position
  , _sampleY     :: {-# UNPACK #-} !Float -- ^ Vertical position
  , _sampleAlpha :: {-# UNPACK #-} !Float -- ^ Alpha
  , _sampleH     :: {-# UNPACK #-} !Float -- ^ Height
  }
\end{lstlisting}
This decomposition follows a divide-and-conquer strategy. For example, decomposing a line
involves repeatedly chopping the line in half until both endpoints are in the same pixel.

This pattern seems ripe for parallelisation, however there are a couple of shortfalls:
\begin{itemize}
\item The size of the canvas severely restricts the length of a primitive -- there is no reason to ever
   have a million pixel line.
 \item The decomposition functions returns the following lazy data structure:
   \begin{lstlisting}
     type Producer a = [a] -> [a]\end{lstlisting}
   presumably to save the cost of generating a long list of lists just to concatenate them in the end
    when the \texttt{EdgeSamples} are sorted.
  \end{itemize}
  Nevertheless, we have experimented with parallelizing the decomposition of primitives.

\subsubsection{Lines}
The \texttt{Graphics.Rasterific.Line} module has the \texttt{decomposeLine} function with type
signature:
\begin{lstlisting}
decomposeLine :: Line -> Producer EdgeSample\end{lstlisting}
The basic work this function does, is decompose a \texttt{Line} primitive into a list of
\texttt{EdgeSample}. As stated above, it does so by divide-and-conquer.

We implemented a case in the internal helper function which sparks a computation for each half of the list.
We experimented a bit with how to chunk the parallelism, and ended up with an option which initially splits
 work and then runs sequentially until depth $n$. This can be seen in Figure \ref{lst:linepar} for $n=10$.
\begin{lstlisting}[caption={Naively splitting work in two parts every 10 levels of the recursion tree.}, label={lst:linepar}]
    go !ax !ay !bx !by n cont
    | n == 0  = runPar $ do
        a' <- spawnP $ go ax ay mx my 10 []
        b' <- spawnP $ go mx my bx by 10 []
        b <- get b'
        a <- get a'
        return $ a ++ b ++ cont
\end{lstlisting}
This approach proves to not generate enough work for each thread. This is even when only a single split
is performed ($n$ is set very large). Figure \ref{fig:line-thread} shows ThreadScope out put for only splitting the
work a single time (per line) on two cores. Even with this course distribution of work
26731 of 34000 sparks end up being garbage collected, and we only manage approximately utilize 25\% of two available
 cores.

Additionally, running the parallel code with a single core (\texttt{-N1}) gives better performance than with
two.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{../threadscope/lines/single-split}
  \caption{ThreadScope output for parallelising line decomposition. Shows the output for the
    \texttt{lines} benchmark.}
  \label{fig:line-thread}
\end{figure}

Inspecting the spark times (Figure \ref{fig:line-thread-sparks}) in ThreadScope confirms that line
decomposition does not produce work enough to justify the hassle --
probably not even to offset any cache locality reduction from working 
on multiple cores. Most of the converted sparks are completed instantaniously.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{../threadscope/lines/single-split-spark-times}
  \caption{ThreadScope spark times for Figure \ref{fig:line-thread}}
  \label{fig:line-thread-sparks}
\end{figure}

To investigate further, we wrote a small program that runs \texttt{decomposeLine} on a single, \textit{unrealisically} large line where
 we fork at every 10th recursive call. Figures \ref{fig:single-line-thread} and \ref{fig:single-line-thread-sparks} show the results. Again we have
 weak utilisation and small spark workloads. Given that this is probably the base case for parallisation of decomposing the line primitive there
  probably isn't must hope for any speedup here.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{../threadscope/lines/single-line-every-10}
  \caption{ThreadScope output for running line decomposition on a single line with length
    100000.}
  \label{fig:single-line-thread}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{../threadscope/lines/single-line-every-10-spark-times}
  \caption{ThreadScope output for running line decomposition on a single line with length
    100000.}
  \label{fig:single-line-thread-sparks}
\end{figure}

\subsection{Sorting Edge Samples}

After the Primitives have been decomposed into \texttt{EdgeSamples}, they are sorted by their y and then x coordinates (in that order).
This is done using the \texttt{vector-algorithms} package, such that the vector of \texttt{EdgeSamples} is sorted in place as a mutable vector
 using an implementation of introsort.
\begin{lstlisting}[caption={Sorting the edge samples.}]
sortEdgeSamples :: [EdgeSample] -> V.Vector EdgeSample
sortEdgeSamples samples = runST $ do
  -- Resist the urge to make this a storable vector,
  -- it is actually a pessimisation.
  mutableVector <- V.unsafeThaw $ V.fromList samples
  VS.sortBy xyCompare mutableVector
  V.unsafeFreeze mutableVector
\end{lstlisting}

Since sparking imperative code is not something you can do by default, and we want spark stats for
analysis, we can make use of a little hack (see Listing \ref{lst:parst}) which creates a sparkable
 thunk out of an ST computation found in an online discussion\footnote{\label{note1}\url{https://unlines.wordpress.com/2010/04/21/sparking-imperatives/}}\footnote{}.
\begin{lstlisting}[label={lst:parst}, caption={\texttt{parST}}]
parST :: ST s a -> ST s a
parST m = x `par` return x
  where
    x = runST (unsafeIOToST noDuplicate >> unsafeCoerce m)
\end{lstlisting}
Obviously, competing with optimized sequential code from the \texttt{vector-algorithms} package is a
 bit is a long shot, but for the sake of experiment we wrote a sorting function (based on an example from where we got the \texttt{parST} function),
 it is a parallel, in-place quicksort (Listing \ref{lst:parsort}) with a depth and size threshold for deciding when to bail out to the aforementioned intro sort.

 Figure \ref{fig:sorting-thread} shows ThreadScope output for a isolated test of the sorting function.
 It shows some good pretty alright utilisation of the two cores.

 \begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{../threadscope/sorting/sorting-final}
  \caption{ThreadScope output for sorting 1.000.000 \texttt{EdgeSample}s.}
  \label{fig:sorting-thread}
\end{figure}

Figure \ref{fig:sorting-thread-zoomed}, shows a zoomed in view of the middle segment of Figure \ref{fig:sorting-thread}.
This gives us the impression, once again, that we have pretty good utilisation.

 \begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{../threadscope/sorting/sorting-final-zoom}
  \caption{ThreadScope output for sorting 1.000.000 \texttt{EdgeSample}s.}
  \label{fig:sorting-thread-zoomed}
\end{figure}

This is reflected in some superficial timings (using \texttt{-s}) -- taking 3.2s on one core, 2.2s on two, 1.6s on three, and 1.4s on four. The bad news is that \texttt{MV.sortBy} takes less than a second to sort the same vector.

Even worse, is that loosening up the parallisation threshold (i.e. increasing the spark production)
greatly decreases performance. Removing the thresholds alltogether results in not being able to get
 through the 1.000.000 length vector at all.
 \begin{figure}[h!]
  \centering
  \includegraphics[width=0.6\linewidth]{../threadscope/sorting/sorting-final-sparks}
  \caption{ThreadScope spark output for sorting 1.000.000 \texttt{EdgeSample}s.}
  \label{fig:sorting-thread-sparks}
\end{figure}

Another symptom of this may be the incredibly low spark conversion rate the sorting function has.
Figure \ref{fig:sorting-thread-sparks} shows the spark conversion rate from the run shown in
Figure \ref{fig:sorting-thread}. Only 6 out of 300 sparks are converted, and 159 end up fizzling.


If we remove the threshold and decrease the vector size to 100k, we get the conversion rate in
Figure \ref{fig:sorting-thread-100k-sparks}. We only convert 5 sparks and generate \textit{two hundred thousand}. This should only be possible if we somehow had generated
a completely unbalanced comparison tree. This implies a performance bug (either in the sorting function
 itself or the \texttt{parST} hack), but we have been unable to locate it.
 \begin{figure}[h!]
  \centering
  \includegraphics[width=0.6\linewidth]{../threadscope/sorting/sorting-100k-sparks}
  \caption{ThreadScope spark output for sorting 100.000 \texttt{EdgeSample}s!}
  \label{fig:sorting-thread-100k-sparks}
\end{figure}

Either way, while we weren't able to generate a method able to sort the \texttt{EdgeSample}s faster
than the sequential intro-sort, we have at least show that there is some potential for it. This
 could reached by parallising the \texttt{vector-algorithms} package.
 
\subsection{Combining Edge Samples}

After the \texttt{EdgeSamples} have been sorted, they are converted to \texttt{CoverageSpans} in the function \texttt{combineEdgeSamples}. The sorting ensures that the algorithm runs over the \texttt{EdgeSamples} in row-major order while accumulating state used to compute the emitted \texttt{CoverageSpans}. The conversion is not one-to-one though, but rather serves as a kind of compression, where, depending on the \texttt{EdgeSamples}' coordinates, either zero, one, or two \texttt{CoverageSpans} are emitted. The algorithm's logic means defining a binary operator for a parallel scan or reduce is out of the question\footnote{It is potentially possible to make an associative operator, but there is no right-neutral element.}.

\begin{lstlisting}[caption={Combining the edge samples.}]
combineEdgeSamples :: (Float -> Float) -> V.Vector EdgeSample
                   -> [CoverageSpan]
{-# INLINE combineEdgeSamples #-}
combineEdgeSamples prepareCoverage vec = go 0 0 0 0 0
  where
    !maxi = V.length vec
    go !ix !x !y !a !_h | ix >= maxi =
      [CoverageSpan x y (prepareCoverage a) 1]
    go !ix !x !y !a !h = sub (vec `V.unsafeIndex` ix) where
      sub (EdgeSample x' y' a' h')
        | y == y' && x == x' =
            go (ix + 1) x' y' (a + a') (h + h')
        | y == y' =
            p1 : p2 : go (ix + 1) x' y' (h + a') (h + h')
        | otherwise =
            p1 : go (ix + 1) x' y' a' h'
            where p1 = CoverageSpan x y (prepareCoverage a) 1
                  p2 = CoverageSpan (x + 1) y
                       (prepareCoverage h) (x' - x - 1)
\end{lstlisting}

Upon further inspection, we note that if two adjacent \texttt{EdgeSamples} are not on the same row (i.e. do not share a $y$-coordinate), the state is discarded. In other words, the \texttt{CoverageSpans} for each row can be computed independently.

To compute the rows in parallel, we need to split them up. We experimented with two different ways of achieving this: changing the data structure from a \texttt{Vector}, or computing the indices to each row in the \texttt{Vector}. A third option, which was not attempted,  would be to rewrite the sorting logic to also extract the indices at no extra cost.

\subsubsection{Grouping by List or Map}

In this experiment (see Listing \ref{lst:combinegroup}), we convert the sorted \texttt{Vector} to a list or a map of \texttt{[EdgeSample]s}, where each element is a sorted list of the \texttt{EdgeSamples} belonging to one row. The additional cost of converting back from the \texttt{Vector} again is dwarfed by the cost incurred by sorting the list or map directly. We then use a parallel map over each row and finally concatenate the output into one \texttt{[CoverageSpan]}.

Figure \ref{fig:combinegrouped} shows the utilisation in one run of the \texttt{snowflake} benchmark with 500 snowflakes. The parallelisation benefit is minimal -- a blip on HEC 1 corresponds to the parallel part of rendering one of the snowflakes.

 \begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{../threadscope/combinegrouped/bigflake}
  \caption{ThreadScope output for running the \texttt{snowflake} program with 500 snowflakes.}
  \label{fig:combinegrouped}
\end{figure}

ThreadScope spark information is shown in Figure \ref{fig:combinegrouped-sparks}. About half of the sparks are garbage-collected, implying their work isn't needed.

 \begin{figure}[H]
  \centering
  \includegraphics[scale=1]{../threadscope/combinegrouped/sparks}
  \caption{ThreadScope spark output for running the \texttt{snowflake} program with 500 snowflakes.}
  \label{fig:combinegrouped-sparks}
\end{figure}

Additionally, the sparks are very small, most non-existent in the spark size output. This would indicate that the parallelised workload is too light, e.g. because the \texttt{EdgeSample} rows are not particularly long, or the work is simply not time-consuming enough.

\subsubsection{Grouping by Index}

In this experiment (see Listing \ref{lst:combineindex}), we keep the \texttt{Vector} representation, but make an additional pass over it in which we collect the start and end indices of each row. Similarly to the other approach, we then do a parallel map and a concatenation at the end. This approach shows largely similar performance results to the former, despite not having the overhead of converting between several data structures.

The answer can perhaps be seen in Figure \ref{fig:combineindex-sparks}, which shows the spark output for running the \texttt{snowflake} program. Almost all of the sparks are garbage-collected, yielding no parallelisation benefit.

 \begin{figure}[h!]
  \centering
  \includegraphics[scale=1]{../threadscope/combineindex/sparks}
  \caption{ThreadScope spark output for running the \texttt{snowflake} program with 500 snowflakes.}
  \label{fig:combineindex-sparks}
\end{figure}

\subsection{Multiple Decomposition Calls}

The rasterizer starts by decomposing each \texttt{Primitive} into \texttt{EdgeSamples}, flattening the results into one big \texttt{[EdgeSample]}, using a lazy continuation-based approach to avoid the overhead of concatenation.

As the decompositions are independent of each other, it should be straightforward to call the decomposition function for each \texttt{Primitive} in parallel, although at the cost of the subsequent concatenation. A brief experiment showed that the number of \texttt{Primitives} for the shapes in our benchmarks was fairly low -- the most complex shape is the snowflake, which consists of 148 curves. As such, there is limited to gain from parallelising these calls in the context of our performance measurements, and we did not attempt to implement this parallelisation.

We do not know if the common use case for Rasterific is likely to be much more complex than the snowflake. This is discussed further in Section \ref{discussion}.

\subsection{Multiple FillOrder Calls}

At the top-level, the user uses the library by writing a number of high-level drawing commands. Each of these are interpreted into \texttt{DrawOrders}, which place the texture/colour/environment changing commands into a State monad, with each \texttt{DrawOrder} being an actual command to draw some shape. I.e. we are at the granularity of e.g. a circle or a rectangle, each made up of \texttt{Primitives}.

The \texttt{DrawOrders} are clipped and rasterised, then coloured/textured. This is all implemented monadically within a \texttt{DrawContext}, which is essentially a State monad for describing the changes to the image.

\begin{lstlisting}[caption={Clipping, rasterising, and texturing the \texttt{DrawOrder}.}]
fillWithTexture :: (PrimMonad m, RenderablePixel px)
                => FillMethod
                -> Texture px  -- ^ Color/Texture used for the filling
                -> [Primitive] -- ^ Primitives to fill
                -> DrawContext m px ()
fillWithTexture fillMethod texture els = do
    img@(MutableImage width height _) <- get
    let !mini = V2 0 0
        !maxi = V2 (fromIntegral width) (fromIntegral height)
        !filler = primToPrim . transformTextureToFiller
                  meshToImage texture img
        clipped = foldMap (clip mini maxi) els
        spans = rasterize fillMethod clipped
    lift . mapExec filler $ filter (isCoverageDrawable img) spans
\end{lstlisting}

We note that the calls to \texttt{clip} and \texttt{rasterize} are not directly dependent on the \texttt{DrawContext} aside from extracting the width and the height of the image. We can therefore refactor this function\footnote{Note that there are other similar functions \texttt{fillWithTextureNoAA} and \texttt{fillWithTextureAndMask} which we ignore since they are not used by the benchmarks and are largely the same.} into two parts: one that clips and rasterises, and one that applies texture. The first part can then be mapped in parallel over the \texttt{DrawOrders} (see Listing \ref{lst:fillorder}).

Figure \ref{fig:lines-fillorder} shows the ThreadScope output of running the \texttt{lines} benchmark once with 5000 lines. We use the \texttt{lines} program, because lines are some of the smallest possible \texttt{DrawOrders} (consisting of one \texttt{Primitive}) yet there are many of them to be computed in parallel.

Interestingly enough, the runtime of this program is still dominated by what comes after calling the extracted \texttt{fillOrder}, namely applying textures and drawing the image. We also see what appears to be GHC's generational garbage collection in action. Every time a generation is collected, it is allowed to increase in size by a factor $n$ until next time, the default being $n=2$, as can be seen in the graph. The forceful evaluation of 5000 lines being converted to \texttt{[CoverageSpan]s} is likely what is causing the need for cleaning up unused memory.
% https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC

 \begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{../threadscope/fillorder/lines}
  \caption{ThreadScope output for running the \texttt{lines} program with 5000 lines.}
  \label{fig:lines-fillorder}
\end{figure}

Figure \ref{fig:lines-fillorder-zoom} shows the same ThreadScope output, zoomed in to show some detail.
The utilisation is very irregular with each spike representing a \texttt{DrawOrder} for one line. This would seem like good utilisation if not for the garbage collection stall that follows after each line.

 \begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{../threadscope/fillorder/lines-zoom}
  \caption{Zoomed ThreadScope output for running the \texttt{lines} program with 5000 lines.}
  \label{fig:lines-fillorder-zoom}
\end{figure}

Figure \ref{fig:lines-fillorder-spark} shows the spark output for the same run. We note the number of converted sparks corresponds to the number of lines in the program, indicating that they were all served by sparks. A large number of sparks are garbage-collected or fizzled, however.

 \begin{figure}[H]
  \centering
  \includegraphics[scale=1]{../threadscope/fillorder/lines-spark}
  \caption{ThreadScope spark output for running the \texttt{lines} program with 5000 lines.}
  \label{fig:lines-fillorder-spark}
\end{figure}

Figure \ref{fig:lines-fillorder-sizes} shows the spark sizes, which seem to be comparatively big. The variation is likely due to the difference in sizes of the lines.

 \begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{../threadscope/fillorder/lines-spark-sizes}
  \caption{ThreadScope spark sizes output for running the \texttt{lines} program with 5000 lines.}
  \label{fig:lines-fillorder-sizes}
\end{figure}

It seems for a program with many \texttt{DrawOrders}, there is a good deal of performance to be gained by parallelising at the \texttt{fillOrder} level, but it comes at a cost -- the forced evaluation of thousands of values puts a strain on the runtime memory and triggers more frequent, and time-consuming, garbage collection. It should be possible to mitigate this by limiting the amount of parallelisation.


\subsection{Results}

lines 1000, snowflakes 500, bigsquare
run with -lfs -N2

master (seq)
\begin{verbatim}
lines: 572.6 ms, std dev = 4.488 ms, 
snowflake: 740.4 ms, 2.149 ms
bigsquare: 179.3 ms, 1.117 ms
\end{verbatim}

fillorder 3.6
\begin{verbatim}
lines: 575.4 ms, 2.656 ms
snowflake: 769.8 ms, 2.255 ms
bigsquare: 183.3 ms, 1.453 ms
\end{verbatim}

sorting 3.3
\begin{verbatim}
lines: 970.8 ms, 6.526 ms
snowflake: 1.061 s, 34.46 ms
bigsquare: 205.2 ms, 3.414 ms
\end{verbatim}

combine (grouped list) 3.a
\begin{verbatim}
lines: 777.9 ms, 377.4 mikros
snowflake: 862.1 ms, 4.795 ms
bigsquare: 214.2 ms, 4.614 ms
\end{verbatim}

combine (vector index) 3.b
\begin{verbatim}
lines: 769.8 ms, 1.482 ms
snowflake: 877.3 ms, 2.500 ms
bigsquare: 198.5 ms, 1.644 ms
\end{verbatim}

\section{Discussion}\label{discussion}

Discussion of results: why are the results (good/)bad in our specific implementations? (Overhead from parallel execution, garbage collection, especially in the non-parallel parts of the code.)
Are the benchmarks good indicators for performance gained by parallelism (...probably)? Should we have used another parallelization technique (i.e. not Par monad)?

% Perhaps use this link somewher https://www.reddit.com/r/haskell/comments/747zx1/parallel_computation_in_st/
Broader discussion: what is the actual scope for parallelisation in rasterific? This is kind of an analysis we could have done before starting, but the experimental approach has its own strengths.

Add some evidence for what the scope of parallelising the specific stuff from Section \label{experiments} -- e.g. traced threadscope graphs, debug traces showing actual numbers of draworders, primitives, and edgesamples generated by our benchmarks, etc. All in all, it's very nested.

\section{Further Work}\label{furtherwork}
The work we have done for this report has been somewhat superficial constrained by the existing flow and data structures in Rasterific.
 It would be interesting to see what could be done if these constraints were up for change. Some of the following ideas could be interesting for future work.
\begin{itemize}
\item Shift Rasterific from relying on laziness, and try to unify which data strucures are used throughout the library to make it more easy to construct
  broad parallel solutions. Perhaps take a look at the Repa package.
\item Try to flatten the calls into the inner rasterization pipeline. This could perhaps enable more course grained parallelism which could be more readily exploited.
  This may require changes to the public API, but may be worth it.
\item While not fully related, we stumbled on to a discussion\footnote{\url{https://www.reddit.com/r/haskell/comments/747zx1/parallel_computation_in_st/}}
  about parallel in-place sorting in Haskell using the ST monad.
  It would be interesting to try to implement a parallel sorting library and see if it is worth it.
  
\item Even for smaller modular attempts at parallelising, our efforts are not exhaustive. There are other places where it could be applied, such as for clipping, which is done just before rasterising. The clipping functions actually return difference lists and so would be much cheaper to concatenate, once computed.
\end{itemize}

\section{Conclusion}
\clearpage
\appendix

\section{Code}
\begin{lstlisting}[label={lst:parsort}, caption={Parallel Sorting}]
parSort :: (Show e, MV.MVector v e, Ord e) => v s e -> ST s ()
parSort a = go a $ ceiling $ log fl
  where
    fl = fromIntegral $ MV.length a
    go a d
      | n < 2 = return ()
      | d < 1 || n < 1000 = VS.sort a -- Don't bother sparking for smaller Vs
      | otherwise = do
          MV.unsafeSwap a 0 mid -- Pivot index > 0
          p <- MV.unsafeRead a 0 -- Get pivot
          let rest = MV.unsafeSlice 1 (n-1) a -- Keep pivot in place
          m' <- MV.unstablePartition (<p) rest -- Partition rest of array
          MV.unsafeSwap a 0 m' -- Swap pivot back into place m' is the first index of the second partition of rest but the last of a
          let a1 = MV.unsafeSlice 0 m' a -- {abcd}p   first slice cannot be empty; second can
          let a2 = MV.unsafeSlice (min (m'+1) n) (n-(min (m'+1) n)) a -- abcdp{efgh}
          v <- parST $ go a1 (d-1)
          v2 <- parST $ go a2 (d-1)
          v `seq` v2 `seq` return ()
            where
              n = MV.length a
              mid = n `div` 2
\end{lstlisting}%$


\begin{lstlisting}[label={lst:sorting}]
main :: IO ()
main = do
  let xs = [x | x <- [1..]] :: [Int]
      ys = [y*2 | y <- [6..]] :: [Int]
      list = take 100 [x `mod` 11 | x <- xs]
      edgesamples = [EdgeSample { _sampleX = fromIntegral  (x `mod` 10),
                                  _sampleY = fromIntegral (y `mod` 30),
                                  _sampleAlpha = (1.5::Float),
                                  _sampleH = (2.1 :: Float)
                                }
                    | x <- xs,
                      y <- ys]
      pa = take 1000000 edgesamples
  start <- getCPUTime
  let sortedPar = sortEdgeSamples pa
  end <- sortedPar `seq` getCPUTime
  let time = (fromIntegral (end - start)) / (10^12)
  printf "Time: %0.9f sec\n" (time :: Double)
\end{lstlisting}

\begin{lstlisting}[caption={Decomposing a very long single line.}, label={lst:singleline}]
import Internal (
  decomposeLine
  ,Line
  )
import Graphics.Rasterific
import Control.DeepSeq
import Debug.Trace

main :: IO ()
main = do
  let l = Line (V2 0 0) (V2 1000000 1000000)
      x = (decomposeLine l) []
  putStrLn $ deepseq x "done"
\end{lstlisting}

\begin{lstlisting}[label={lst:combinegroup}, caption={combineEdgeSamples with grouping}]
sortAndCombineEdgeSamples2 :: (Float -> Float) -> [EdgeSample] -> [CoverageSpan]
sortAndCombineEdgeSamples2 f = combineEdgeSamples2 f . sortEdgeSamplesList2

combineEdgeSamples2 :: forall (t :: * -> *) . Traversable t
                   => (Float -> Float) -> t [EdgeSample] -> [CoverageSpan]
{-# INLINE combineEdgeSamples2 #-}
combineEdgeSamples2 prepareCoverage =
  concat . runPar . parMap gogo
  where
    gogo (EdgeSample a b c d:as) = go a b c d as
    -- all edge samples in a sublist have the same y coordinate
    go !x !y !a !_h [] = [CoverageSpan x y (prepareCoverage a) 1]
    go !x !y !a !h (EdgeSample x' y' a' h' : vs)
      | x == x' = go x' y' (a+a') (h+h') vs
      | otherwise = p1 : p2 : go x' y' (h+a') (h+h') vs
      where
        p1 = CoverageSpan x y (prepareCoverage a) 1
        p2 = CoverageSpan (x+1) y (prepareCoverage h) (x'-x-1)

-- map-based grouping
sortEdgeSamplesMap2 :: [EdgeSample] -> Map Float [EdgeSample]
sortEdgeSamplesMap2 samples =
  fromAscListWith (++) [(_sampleY a, [a]) | a <- sorted]
  where sorted = V.toList $ sortEdgeSamples1 xyCompareR samples

-- list-based grouping
sortEdgeSamplesList2 :: [EdgeSample] -> [[EdgeSample]]
sortEdgeSamplesList2 samples =
  groupBy (\a b -> _sampleY a == _sampleY b) sorted
  where sorted = V.toList $ sortEdgeSamples1 xyCompare samples
\end{lstlisting}


\begin{lstlisting}[label={lst:combineindex}, caption={combineEdgeSamples with indexing}]
sortAndCombineEdgeSamples1 :: (Float -> Float) -> [EdgeSample] -> [CoverageSpan]
sortAndCombineEdgeSamples1 f =
  combineEdgeSamples1 f . sortEdgeSamples1 xyCompare

combineEdgeSamples1 :: (Float -> Float) -> V.Vector EdgeSample -> [CoverageSpan]
{-# INLINE combineEdgeSamples1 #-}
combineEdgeSamples1 prepareCoverage samples =
  concat $ runPar $ parMap (go 0 0 0 0) yind
  where
    !maxi = V.length samples
    !hd = samples `V.unsafeIndex` 0
    -- (start, end) index pairs of each y-segment
    (_,_,yind) =
      V.ifoldl'
      (\(y,o,acc) i e ->
        let y' = _sampleY e in
        if i/=maxi-1 && y'==y then (y,o,acc)
        else if y'/=y then (y',i,(o,i):acc)
        else (y',i,(o,maxi):acc) --last element
      )
      (_sampleY hd,0,[])
      samples
    go !x !y !a !_h (!s, !e) | s == e = [CoverageSpan x y (prepareCoverage a) 1]
    go !x !y !a !h (!ix, !e) = sub (samples `V.unsafeIndex` ix) where
      sub (EdgeSample x' y' a' h')
        | x == x' = go x' y' (a + a') (h + h') (ix+1,e)
        | otherwise = p1 : p2 : go x' y' (h + a') (h + h') (ix+1,e)
             where p1 = CoverageSpan x y (prepareCoverage a) 1
                   p2 = CoverageSpan (x + 1) y (prepareCoverage h) (x' - x - 1)

sortEdgeSamples1 :: (EdgeSample -> EdgeSample -> Ordering)
                    -> [EdgeSample] -> V.Vector EdgeSample
sortEdgeSamples1 cmp samples = runST $ do
    -- Resist the urge to make this a storable vector,
    -- it is actually a pessimisation.
    mutableVector <- V.unsafeThaw $ V.fromList samples
    VS.sortBy cmp mutableVector
    V.unsafeFreeze mutableVector
\end{lstlisting}

\begin{lstlisting}[label={lst:fillorder}, caption={Parallel fillOrder}]
-- in Rasterific.hs
renderDrawingAtDpi
    :: forall px . (RenderablePixel px)
    => Int -- ^ Rendering width
    -> Int -- ^ Rendering height
    -> Dpi -- ^ Current DPI used for text rendering.
    -> px  -- ^ Background color
    -> Drawing px () -- ^ Rendering action
    -> Image px
renderDrawingAtDpi width height dpi background drawing =
    let m = drawOrdersOfDrawing width height dpi background drawing
        k = runPar $ parMap (fillOrder1 width height) m
    in runST $ runDrawContext width height background $ mapM_ fillOrder2 $ zip m k

----

fillOrder1 :: (Integral x, RenderablePixel px)
           => x -> x -> DrawOrder px -> [[CoverageSpan]]
fillOrder1 width height o@DrawOrder { _orderMask = Nothing } =
  runPar $ parMap (fillWithTexture1 (_orderFillMethod o) width height) (_orderPrimitives o)
fillOrder1 _ _ _ = error "not implemented"

fillOrder2 :: (PrimMonad m, RenderablePixel px)
           => (DrawOrder px, [[CoverageSpan]]) -> DrawContext m px ()
fillOrder2 (o, spans) = do
  F.forM_ spans $ fillWithTexture2 (_orderTexture o)
  img <- get
  lift $ primToPrim $ flip evalStateT img $ _orderDirect o

----

fillWithTexture1 :: Integral x
                 => FillMethod
                 -> x -> x
                 -> [Primitive] -- ^ Primitives to fill
                 -> [CoverageSpan]
fillWithTexture1 fillMethod width height els =
    let !mini = V2 0 0
        !maxi = V2 (fromIntegral width) (fromIntegral height)
        clipped = foldMap (clip mini maxi) els
        spans = rasterize fillMethod clipped
    in spans

fillWithTexture2 :: (PrimMonad m, RenderablePixel px)
                 => Texture px -> [CoverageSpan] -> DrawContext m px ()
fillWithTexture2 texture spans = do
    img <- get
    let !filler = primToPrim . transformTextureToFiller meshToImage texture img in
      lift . mapExec filler $ filter (isCoverageDrawable img) spans
\end{lstlisting}

\end{document}
